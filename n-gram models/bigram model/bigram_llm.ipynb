{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b1e312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA Available: False\n",
      "GPU Count: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92764eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145414\n"
     ]
    }
   ],
   "source": [
    "# Choosing a text file to train the bigram model on.\n",
    "with open('Alice_In_Wonderland.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# Printing some stuff about the text, like its length.\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c013775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '5', '6', '7', '8', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ù', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to make a \\\"characters\\\" variable to create a vocabulary list.\n",
    "characters = sorted(list(set(text)))\n",
    "print(characters)\n",
    "# We can even see how many unique characters there are in the text.\n",
    "print(len(characters))\n",
    "# As you can see, there are 84 unique characters in the text.\n",
    "vocab_size = len(characters) # Time to give this number a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218082c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([83, 41, 58, 55,  1, 37, 68, 65, 60, 55, 53, 70,  1, 28, 71, 70, 55, 64,\n",
      "        52, 55, 68, 57,  1, 55, 23, 65, 65, 61,  1, 65, 56,  1, 22, 62, 59, 53,\n",
      "        55,  4, 69,  1, 22, 54, 72, 55, 64, 70, 71, 68, 55, 69,  1, 59, 64,  1,\n",
      "        44, 65, 64, 54, 55, 68, 62, 51, 64, 54,  0,  1,  1,  1,  1,  0, 41, 58,\n",
      "        59, 69,  1, 55, 52, 65, 65, 61,  1, 59, 69,  1, 56, 65, 68,  1, 70, 58,\n",
      "        55,  1, 71, 69, 55,  1, 65, 56,  1, 51])\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to make a tokenizer that maps characters to integers.\n",
    "# The code below achieves this by creating two dictionaries: one for mapping characters to integers and another for mapping integers back to characters. \n",
    "# It also defines two functionsfor encoding strings to lists of integers and the reverse process.\n",
    "string_to_int = {ch: i for i, ch in enumerate(characters)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(characters)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# If we work with, maybe, a word-level model, we would want to tokenize by words instead of characters.\n",
    "# With a word-level tokenizer, we would have, for example, every single word in the English language as a unique token.\n",
    "# That's a lot of tokens, and for a multiple language model, we would have millions, billions, or trillions of tokens.\n",
    "# But in that case, we would have a much smaller set to work with, resulting in a large vocabulary size and a very small amount to encode and decode.\n",
    "# If you have a subword tokenizer, you would have a smaller vocabulary size than a word-level tokenizer, but a larger vocabulary size than a character-level tokenizer.\n",
    "# In the context, of language models, it's really important that we are efficient with our data and that we have a good tokenizer that can capture the structure of the language while also being efficient in terms of the number of tokens it produces.\n",
    "# It should also be noted that we will be using a machine learning framework called PyTorch to build this bigram model.\n",
    "# We're going to add our data in now.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# Let's print the data to see what it looks like.\n",
    "print (data [:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61b65a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 116331\n",
      "Validation data length: 29083\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to split the data into a training set and a validation set.\n",
    "# n is the index at which we will split the data. We will use 80% of the data for training and 20% for validation.\n",
    "# The general rule for splitting data is to use 80% for training and 20% for validation, based on Pareto principle.\n",
    "# However, the optimal split can depend on the size of the dataset and the specific task at hand.\n",
    "# In this case, using an 80/20 split should work well for training this bigram model.\n",
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Training data length: {len(train_data)}\")\n",
    "print(f\"Validation data length: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d7cbf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([83]), Target: 41\n",
      "Input: tensor([83, 41]), Target: 58\n",
      "Input: tensor([83, 41, 58]), Target: 55\n",
      "Input: tensor([83, 41, 58, 55]), Target: 1\n",
      "Input: tensor([83, 41, 58, 55,  1]), Target: 37\n",
      "Input: tensor([83, 41, 58, 55,  1, 37]), Target: 68\n",
      "Input: tensor([83, 41, 58, 55,  1, 37, 68]), Target: 65\n",
      "Input: tensor([83, 41, 58, 55,  1, 37, 68, 65]), Target: 60\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to make a function that will generate batches of data for training the model.\n",
    "# The function will take in the batch size and the block size as arguments.\n",
    "# We will take a set of predictions and offset the targets by one character.\n",
    "# This way, the model will learn to predict the next character in the sequence.\n",
    "block_size = 8  # The block size is the number of characters the model will look at to make a prediction.\n",
    "\n",
    "x = train_data[:block_size]  # The input data will be the first block of characters.\n",
    "y = train_data[1:block_size+1]  # The target data will be the next block of characters, offset by one.\n",
    "\n",
    "for i in range(block_size):\n",
    "    input = x[:i+1]  # The input will be the characters up to the current index. This is also called the \"context\".\n",
    "    target = y[i]  # The target will be the character at the current index.\n",
    "    print(f'Input: {input}, Target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8090aeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[82,  1, 69, 51, 59, 54,  1, 70],\n",
      "        [ 1, 59, 70,  0, 56, 55, 62, 70],\n",
      "        [55,  1, 66, 59, 55, 53, 55, 69],\n",
      "        [58, 59, 64, 57,  1, 73, 65, 68]])\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 69, 51, 59, 54,  1, 70, 58],\n",
      "        [59, 70,  0, 56, 55, 62, 70,  1],\n",
      "        [ 1, 66, 59, 55, 53, 55, 69,  1],\n",
      "        [59, 64, 57,  1, 73, 65, 68, 70]])\n"
     ]
    }
   ],
   "source": [
    "# Now we can take these blocks, stack them, and push them to the GPU to scale up the training process.\n",
    "# We can represent this stack of blocks as a new hyperparameter called \"batch size\".\n",
    "# First, let's check if we have a GPU available and set the device accordingly.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "# Looks like we need to find an available GPU.\n",
    "# How to find an available GPU? \n",
    "# You can use the command \"nvidia-smi\" in your terminal to check the status of your GPUs. \n",
    "# It will show you the memory usage and the processes running on each GPU. \n",
    "# You can choose a GPU that has low memory usage and is not being heavily used by other processes.\n",
    "# Since GPU is not available, so we will be using the CPU for training.\n",
    "batch_size = 4\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # If we had a GPU, this would move the data to the GPU for parallel processing.\n",
    "    return x, y\n",
    "x, y = get_batch('train')\n",
    "print(\"Inputs: \")\n",
    "print(x.shape)  # This will show the shape of the input batch, which should be (batch_size, block_size).\n",
    "print(x)\n",
    "print(\"Targets: \")\n",
    "print(y.shape)  # This will show the shape of the target batch, which should also be (batch_size, block_size).\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6965f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # This tells PyTorch not to compute gradients, which is useful for inference and evaluation.\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()  # This sets the model to evaluation mode, which is important for certain layers like dropout and batch normalization.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()  # This sets the model back to training mode.\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b46323cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EmDEeR*.:jQU3ai?wZ-﻿’8(us-‘e1mo’Brkf(WP*F#8BM2’uLGh8Em6.Fw_c﻿'*)wObQ”Pz3gvT ”Br‘ts_3'Dx)vMd!G]‘7“0q!Q5(ùY16'pdaKvYw_5dJy)“v5-—Yv16fUO(l!ML)6yqDCru,\n",
      "Zic(UM#mz*U32’]“Nu﻿g8X\n",
      "'PO6b!nMjI#7‘3L!MzCu*QJTb\n",
      "j;sLzofaZeZK’R8xKHt;'Q*(yHKS6c)r—kKHFp]t0E—Uf[U“!0ùo]FrSg[fsn.h_Es2w\n",
      ")“l].?7k8(o]D.x w\n",
      "s[F8fU)!onK\n",
      "ES0kt[”YKS‘Nw_)rc18”8;)f_e”Tù Yk1z;X﻿BiHAs:02GGYUYG“;5AUZg‘UeW]Dy”5 R—.3-m3,UY‘JaC]ùe1nU86RLgu1hfs5ioesf\n",
      "K]rLrq’3So’aErJ,MWP.ù2Tt_WOKm’rs5ZaGùIdnYSKe-*U).-﻿xJ*WPpMeZ#.F#x(s[\n",
      ")Zb,N#8’[D”’uJ#KvfO6cu:“Fgyj;ù\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to use gradient descent to train the model.\n",
    "# PyTorch has a variety of optimizers that we can use for this purpose.\n",
    "# We are going to use the AdamW optimization algorithm.\n",
    "# Adam is an optimization algorithm that uses a moving average of the gradient and its square value to adapt the learning rate of each parameter.\n",
    "# AdamW is a variant of the Adam optimizer that decouples weight decay from the gradient update, which can lead to better performance in some cases.\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # This creates an embedding table that maps each token to a vector of the same size as the vocabulary.\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)  # Logits are a bunch of floating point numbers that are normalized in a probability distribution. They represent the model's predictions for the next character in the sequence.\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # B is the batch size, T is the time dimension, and C is the channel dimension. The time dimension is named as such because there are some tokens that we do know and some that we do not know at a given time step. Channels are just how many channels we are going to have, which would be our vocab size.\n",
    "            logits = logits.view(B*T, C) # We are paying attention to the channel dimension, so we can blend the batch and time dimensions together.\n",
    "            targets = targets.view(B*T) # The targets will have the same batch and time dimensions, which should be alright.\n",
    "            loss = F.cross_entropy(logits, targets) # Cross entropy loss is a common loss function used for classification problems. It measures the difference between the predicted probability distribution and the true distribution.\n",
    "            # We essentially reshaped this data because PyTorch expects this shape to be a (N, C) shape, where N is the number of samples and C is the number of classes.\n",
    "            # When passing the logits into a functional, it is important to understand what shape PyTorch expects the data to be in, and how to reshape the data to fit that shape.\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # Index is (B, T) shape, where B is the batch size and T is the time dimension. This represents the current context of tokens that we have generated so far.\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # Specifying the last time step creates (B, C) shape.\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # Softmax is a function that converts logits into probabilities. It takes the exponent of each logit and normalizes them so that they sum to 1. (B, C) shape.\n",
    "            # Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)  # Multinomial sampling is a way to sample from a probability distribution. It takes the probabilities and samples an index based on those probabilities. (B, 1) shape.\n",
    "            # Append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1)  # (B, T+1) shape.\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device) # This will move the model to the GPU if it is available, otherwise it will stay on the CPU.\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # This is the initial context for generation. It starts with a single token (the start token) and will be updated as we generate new tokens.\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist()) # This will generate a sequence of 500 new tokens based on the initial context and decode it back into characters.\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28572052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Train loss: 2.676, Val loss: 2.710\n",
      "Step: 250 Train loss: 2.655, Val loss: 2.683\n",
      "Step: 500 Train loss: 2.670, Val loss: 2.683\n",
      "Step: 750 Train loss: 2.655, Val loss: 2.689\n",
      "Step: 1000 Train loss: 2.669, Val loss: 2.664\n",
      "Step: 1250 Train loss: 2.614, Val loss: 2.638\n",
      "Step: 1500 Train loss: 2.639, Val loss: 2.658\n",
      "Step: 1750 Train loss: 2.620, Val loss: 2.635\n",
      "Step: 2000 Train loss: 2.621, Val loss: 2.620\n",
      "Step: 2250 Train loss: 2.608, Val loss: 2.645\n",
      "Step: 2500 Train loss: 2.622, Val loss: 2.621\n",
      "Step: 2750 Train loss: 2.627, Val loss: 2.615\n",
      "Step: 3000 Train loss: 2.580, Val loss: 2.615\n",
      "Step: 3250 Train loss: 2.596, Val loss: 2.609\n",
      "Step: 3500 Train loss: 2.608, Val loss: 2.606\n",
      "Step: 3750 Train loss: 2.589, Val loss: 2.615\n",
      "Step: 4000 Train loss: 2.583, Val loss: 2.599\n",
      "Step: 4250 Train loss: 2.606, Val loss: 2.575\n",
      "Step: 4500 Train loss: 2.567, Val loss: 2.579\n",
      "Step: 4750 Train loss: 2.573, Val loss: 2.566\n",
      "Step: 5000 Train loss: 2.553, Val loss: 2.569\n",
      "Step: 5250 Train loss: 2.561, Val loss: 2.579\n",
      "Step: 5500 Train loss: 2.567, Val loss: 2.595\n",
      "Step: 5750 Train loss: 2.565, Val loss: 2.585\n",
      "Step: 6000 Train loss: 2.571, Val loss: 2.567\n",
      "Step: 6250 Train loss: 2.557, Val loss: 2.582\n",
      "Step: 6500 Train loss: 2.556, Val loss: 2.548\n",
      "Step: 6750 Train loss: 2.557, Val loss: 2.575\n",
      "Step: 7000 Train loss: 2.541, Val loss: 2.573\n",
      "Step: 7250 Train loss: 2.525, Val loss: 2.557\n",
      "Step: 7500 Train loss: 2.549, Val loss: 2.560\n",
      "Step: 7750 Train loss: 2.530, Val loss: 2.590\n",
      "Step: 8000 Train loss: 2.526, Val loss: 2.551\n",
      "Step: 8250 Train loss: 2.522, Val loss: 2.579\n",
      "Step: 8500 Train loss: 2.540, Val loss: 2.544\n",
      "Step: 8750 Train loss: 2.536, Val loss: 2.508\n",
      "Step: 9000 Train loss: 2.518, Val loss: 2.562\n",
      "Step: 9250 Train loss: 2.527, Val loss: 2.526\n",
      "Step: 9500 Train loss: 2.544, Val loss: 2.521\n",
      "Step: 9750 Train loss: 2.512, Val loss: 2.536\n",
      "2.4488158226013184\n"
     ]
    }
   ],
   "source": [
    "# Create the optimization algorithm and the learning rate.\n",
    "learning_rate = 3e-4 # Picking a learning rate. A common learning rate is 3e-4, but you can experiment with different learning rates to test the performance of the model.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # As discussed, we will use the AdamW optimization algorithm.\n",
    "\n",
    "max_iters = 10000  # Define number of training iterations. 10000 iterations is generally good for this case, but you can experiment with different values to test the performance of the model.\n",
    "\n",
    "# We can even add loss reporting with /\"eval_iters/\" to see how the model is improving during training.\n",
    "eval_iters = 250\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step: {iter} Train loss: {losses[\"train\"]:.3f}, Val loss: {losses[\"val\"]:.3f}')\n",
    "    # Sample a batch of data\n",
    "    x, y = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model.forward(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True) # This will reset the gradients of the model parameters to zero before we perform backpropagation. This is important because PyTorch accumulates gradients by default, so we need to clear them out before each optimization step.\n",
    "    loss.backward() # This will perform backpropagation and compute the gradients of the loss with respect to the model parameters.\n",
    "    optimizer.step() # This will update the model parameters based on the computed gradients and the optimization algorithm.\n",
    "print(loss.item()) # This will print the final loss after training the model. The .item() method is used to get the scalar value of the loss tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d0458ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "So hed_ lind y fup,” Qn blitotrd.” t caud\n",
      "nggalopenr me ase; Aliliserded—_m’se howe welale\n",
      "\n",
      "QJmith, she d s, f  lof smarelit f)“If as?Nke whede as\n",
      "ntheno is y!﻿7in “R*jUN!, y cte, d dlderend ve\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "f hash, s tomeeirt pus t dy het ar—“On’ru?Cr s k’lbans “orrue won shind, wnog toupuzJ[D2GTh,\n",
      "‘3f seticend imette d—”'f she tab0OJI o targooaypphelit C)fo?_[Mitit, ajityandeB—” erevllldngoowithere t Athit _y ssplllped heed ain he herovend  ifenothelony d ry, esed fuff-‘-﻿—YK0#5Nbsassheaien wnt, ouitlor\n"
     ]
    }
   ],
   "source": [
    "# We run the same generation code as before to see how the model has improved after training.\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
